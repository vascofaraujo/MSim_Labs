
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Lab4_Final</title><meta name="generator" content="MATLAB 9.4"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2020-06-03"><meta name="DC.source" content="Lab4_Final.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1></h1><!--introduction--><p>LAB 4 - MSIM Autor: Bernardo Rocha &amp; Vasco Ara&uacute;jo N&uacute;mero de Aluno: 89867 &amp; 90817 Junho 2020; &Uacute;ltima Revis&atilde;o: 03/06/2020</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Exerc&iacute;cio 2.a</a></li><li><a href="#3">Exerc&iacute;cio 2.b</a></li><li><a href="#5">Exerc&iacute;cio 2.c</a></li><li><a href="#7">Exerc&iacute;cio 2.d</a></li><li><a href="#9">Exerc&iacute;cio 3a e 3b</a></li><li><a href="#11">Exerc&iacute;cio 3c</a></li><li><a href="#12">Functions - part3Exe</a></li><li><a href="#13">Functions - execute</a></li><li><a href="#14">Functions - PowrGen</a></li></ul></div><h2 id="1">Exerc&iacute;cio 2.a</h2><pre class="codeinput">close <span class="string">all</span>
clear <span class="string">all</span>
clc

MarkovChain = load(<span class="string">'MarkovChain.mat'</span>);

P = MarkovChain.P;

[vectors, values] = eig(P');

old_diference = 10000;
val_u = 1;
<span class="comment">%encontra o valor proprio mais perto de 1</span>
<span class="keyword">for</span> i = 1:size(values,1)
    diference = abs(values(i,i) - 1);
    <span class="keyword">if</span> diference &lt; old_diference
        old_diference = diference;
        value_index = i;
    <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="comment">%vector a normalizar</span>
vec_n = vectors(:,value_index);

normalize = vec_n/sum(vec_n);

<span class="comment">%se esta tudo bem soma das probabilidades tem que dar 1</span>
prob_sum = sum(normalize)

<span class="comment">%faz gr&aacute;fico de barras</span>
figure(1)
bar(normalize);
title(<span class="string">'Distribui&ccedil;&atilde;o de equil&iacute;brio da cadeia de Markov'</span>);
xlabel(<span class="string">'N&uacute;mero do estado da cadeia de Markov'</span>);
ylabel(<span class="string">'Probabilidade do token estar na posse do agente'</span>);
box <span class="string">on</span>;
grid <span class="string">on</span>;

<span class="comment">%encontra estado mais provavel</span>
high_prob = 0;
low_prob = 1;
<span class="comment">%encontra o 1&ordm; estado mais provavel e o 1&ordm; estado menos provavel</span>
<span class="keyword">for</span> i = 1:size(normalize,1)
    <span class="keyword">if</span> normalize(i) &gt; high_prob
        high_prob = normalize(i);
    <span class="keyword">end</span>
    <span class="keyword">if</span> normalize(i) &lt;= low_prob
        low_prob = normalize(i);
    <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="comment">%volta a correr para ver se ha mais estados com mesma probabilidade que o</span>
<span class="comment">%mais provavel e o menos provavel</span>
h = 1;
l = 1;
<span class="keyword">for</span> i = 1:size(normalize,1)
    <span class="keyword">if</span> normalize(i) == high_prob
        highest_prob_index(h) = i;
        highest_prob(h) = normalize(i)
        highest_prob_anchor = i
        h = h + 1;
    <span class="keyword">end</span>
    <span class="keyword">if</span> normalize(i) == low_prob
        lowest_prob_index(l) = i;
        lowest_prob(l) = normalize(i)
        lowest_prob_anchor = i
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><pre class="codeoutput">
prob_sum =

    1.0000


highest_prob =

    0.0965


highest_prob_anchor =

     7


lowest_prob =

    0.0107


lowest_prob_anchor =

     8

</pre><img vspace="5" hspace="5" src="Lab4_Final_01.png" alt=""> <p>Olhando para o gr&aacute;fico de barras poderia pensar-se que os estados mais prov&aacute;veis eram os estados 7 e 19, no entanto correndo o c&oacute;digo para encontrar os maiores estados apenas no d&aacute; o estado 7. Isto &eacute; porque, analisando os valores de normalize verifica-se que o estado 7 tem uma probabilidade de 0.096493187402279 e o estado 19 uma probabilidade de 0.096493187402278, por isso o c&oacute;digo apenas retorna 7 como o estado com maior probabilidade. O mesmo acontece com os estados 8 e 17 para o c&aacute;lculo do estado com menor probabilidade, visto que o estado 17 tem uma probabilidade um pouco menor. Assim sendo, se quiseremos ser rigorosos o estado com maior probabilidade &eacute; o estado 7 e o estado com menor probabilidade &eacute; o estado 8.</p><h2 id="3">Exerc&iacute;cio 2.b</h2><pre class="codeinput">close <span class="string">all</span>
clear <span class="string">all</span>
clc

MarkovChain = load(<span class="string">'MarkovChain.mat'</span>);

<span class="comment">%par&acirc;metros da simula&ccedil;&atilde;o</span>
N = size(MarkovChain.nodePos,1);        <span class="comment">%numero de ancoras</span>
n = 2;                                  <span class="comment">%numero de dimensoes</span>
sidelength = 100;

M = 10000;                               <span class="comment">%numero de observa&ccedil;&otilde;es</span>

<span class="comment">%posi&ccedil;oes das &acirc;ncoras</span>
a = [MarkovChain.nodePos(:,2) , MarkovChain.nodePos(:,3)]';
<span class="comment">%posi&ccedil;&otilde;es das sources</span>
x = MarkovChain.sourcePos';


D = squareform(pdist([x zeros(size(x)) a]'));
d = D(1,3:end);                     <span class="comment">% Source-anchor distances</span>
an = D(2,3:end);                    <span class="comment">% Anchor norms</span>

P0 = 100;
mu = 0;
sigma = 1e-2;

<span class="comment">%inicializa matrizes</span>
A = zeros(M,4);
b = zeros(M,1);

<span class="comment">%ruido ni</span>
ni_space = [-0.1:.0002:0.1];
ni = normpdf(ni_space, mu, sigma);

<span class="comment">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span>
<span class="comment">%c&oacute;digo da 2.a</span>
P = MarkovChain.P;
[vectors, values] = eig(P');
old_diference = 10000;
val_u = 1;
<span class="comment">%encontra o valor proprio mais perto de 1</span>
<span class="keyword">for</span> i = 1:size(values,1)
    diference = abs(values(i,i) - 1);
    <span class="keyword">if</span> diference &lt; old_diference
        old_diference = diference;
        value_index = i;
    <span class="keyword">end</span>
<span class="keyword">end</span>
<span class="comment">%vector a normalizar</span>
vec_n = vectors(:,value_index);
normalize = vec_n/sum(vec_n);
<span class="comment">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span>

<span class="comment">%numero de vezes que cada &acirc;ncora tem o token</span>
number_meas = normalize.*M;
<span class="comment">%aproxima&ccedil;&atilde;o para o n&uacute;mero inteiro mais pr&oacute;ximo</span>
number_meas = round(number_meas);

<span class="comment">%calcula as matrizes A e b</span>
j = 1;
<span class="keyword">for</span> i = 1:N
    meas = number_meas(i);
    <span class="keyword">if</span> meas ~= 0
        P(j) = (P0./(d(i).^2))*exp(ni(j));
        A(j,1) = -2*P(j)*a(1,i);
        A(j,2) = -2*P(j)*a(2,i);
        A(j,3) = -1;
        A(j,4) = P(j);
        b(j) = -P(j)*(an(i)^2)';
        j = j + 1;
        meas = meas-1;
    <span class="keyword">end</span>
<span class="keyword">end</span>

z = A\b;
fprintf(<span class="string">'Erro com %d mudan&ccedil;as de agente'</span>, M);
x_estimated = z(1:n);
error1 = norm(x-x_estimated)

<span class="comment">% RLS formulation (one-shot)</span>
<span class="comment">% RlsPar = struct('lam',1);</span>
<span class="comment">% [e,w,RlsPar] = qrrls(A,b,RlsPar);</span>
<span class="comment">%error1 = norm(z-w)</span>
<span class="comment">%x_estimated = [w(1) w(2)]';</span>



figure(1)
plot(a'*[1; 1i],<span class="string">'o'</span>); hold <span class="string">all</span>
plot(x'*[1; 1i],<span class="string">'x'</span>);plot(x_estimated'*[1; 1i],<span class="string">'s'</span>); hold <span class="string">off</span>
axis(sidelength*[0 1 0 1]); axis(<span class="string">'square'</span>)
title(<span class="string">'Estima&ccedil;&atilde;o da posi&ccedil;&atilde;o da source'</span>);
legend(<span class="string">'&Acirc;ncoras'</span>, <span class="string">'Posi&ccedil;&atilde;o real da source'</span>, <span class="string">'Posi&ccedil;&atilde;o estimada da source com 10000 mudan&ccedil;as'</span>);
xlabel(<span class="string">'x'</span>);
ylabel(<span class="string">'y'</span>);

figure(2)
hold <span class="string">all</span>
plot(x'*[1; 1i],<span class="string">'x'</span>); plot(x_estimated'*[1; 1i],<span class="string">'s'</span>);
axis([84.9999 85.0001 29.9999 30.0001]);
title(<span class="string">'Estima&ccedil;&atilde;o da posi&ccedil;&atilde;o da source ampliada'</span>);
legend(<span class="string">'Posi&ccedil;&atilde;o real da source'</span>, <span class="string">'Posi&ccedil;&atilde;o estimada da source com 10000 mudan&ccedil;as'</span>, <span class="string">'Location'</span>,<span class="string">'southwest'</span>);
xlabel(<span class="string">'x'</span>);
ylabel(<span class="string">'y'</span>);
</pre><pre class="codeoutput">Erro com 10000 mudan&ccedil;as de agente
error1 =

   1.0658e-14

</pre><img vspace="5" hspace="5" src="Lab4_Final_02.png" alt=""> <img vspace="5" hspace="5" src="Lab4_Final_03.png" alt=""> <p>Decidiu-se usar a primeira formula&ccedil;&atilde;o apresentada no ficheiro 'rssiloc.m' por ser a que apresentava um erro menor. Usando o algoritmo de m&iacute;nimos quedrados com base na solu&ccedil;&atilde;o matricial consegue-se um resultado bastante fiel para a fonte estimada, dado um n&uacute;mero elevado de transi&ccedil;&otilde;es. O erro nunca poderia ser zero devido ao ru&iacute;do gaussiano na medi&ccedil;&atilde;o da pot&ecirc;ncia da fonte, por isso consideramos o nosso erro de 1.0658e-14 bastante bom.</p><h2 id="5">Exerc&iacute;cio 2.c</h2><pre class="codeinput">clear <span class="string">all</span>
close <span class="string">all</span>
clc



MarkovChain = load(<span class="string">'MarkovChain.mat'</span>);
P = MarkovChain.P;

<span class="comment">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span>
<span class="comment">%2.a (para usar mais a frente)</span>
[vectors, values] = eig(P');
old_diference = 10000;
val_u = 1;
<span class="comment">%encontra o valor proprio mais perto de 1</span>
<span class="keyword">for</span> i = 1:size(values,1)
    diference = abs(values(i,i) - 1);
    <span class="keyword">if</span> diference &lt; old_diference
        old_diference = diference;
        value_index = i;
    <span class="keyword">end</span>
<span class="keyword">end</span>
<span class="comment">%vector a normalizar</span>
vec_n = vectors(:,value_index);
normalize = vec_n/sum(vec_n);
<span class="comment">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span>

N = 20;
time = 500;

pi_array = zeros(N,time);

<span class="comment">%inicializar a matriz pi_array_0</span>
<span class="comment">%cada coluna corresponde ao vetor inicial para cada simula&ccedil;&atilde;o</span>
pi_array_0 = zeros(N,4);

<span class="comment">%1&ordm; condi&ccedil;&atilde;o inicial -&gt; come&ccedil;a no estado 7</span>
pi_array_0(7,1) = 1;
<span class="comment">%2&ordm; condi&ccedil;&atilde;o inicial -&gt; come&ccedil;a no estado 8</span>
pi_array_0(8,2) = 1;
<span class="comment">%3&ordm; condi&ccedil;&atilde;o -&gt; todos os estados t&ecirc;m a mesma probabilidade</span>
<span class="keyword">for</span> i = 1:20
    pi_array_0(i,3) = 1/20;
<span class="keyword">end</span>

<span class="keyword">for</span> i = 1:3
    eq_time(i) = 0;
<span class="keyword">end</span>


<span class="keyword">for</span> j = 1:3
    pi_array(:,1) = pi_array_0(:,j);
    <span class="keyword">for</span> i = 2:time
        pi_array(:,i) = pi_array(:,1)'*P^(i);
    <span class="keyword">end</span>

    <span class="comment">%para comparar com os resultados da decomposi&ccedil;&atilde;o em valores e vetores</span>
    <span class="comment">%proprios (2.a) vamos calcular o tempo que demora a atingir o</span>
    <span class="comment">%equil&iacute;brio</span>
    <span class="keyword">for</span> i = 1:time
        <span class="keyword">if</span> pi_array(:,i) - normalize &lt; 0.000001
            eq_time(j) = i;
            <span class="keyword">break</span>;
        <span class="keyword">end</span>
    <span class="keyword">end</span>

    figure(j)
    t_array = 1:1:time;
    anc=repmat(1:1:20, length(t_array), 1);
    plot3(t_array, anc, pi_array);
<span class="keyword">end</span>

<span class="comment">%verificar que para cada t soma de pi(t)= 1</span>
<span class="keyword">for</span> j = 1:3
    <span class="keyword">for</span> i = 1:time
        sum_pi(j,i) = sum(pi_array(:,i));
    <span class="keyword">end</span>
<span class="keyword">end</span>

disp(<span class="string">'Tempo para atingir o equil&iacute;brio para cada condi&ccedil;&atilde;o:'</span>);
condicao1 = eq_time(1)
condicao2 = eq_time(2)
condicao3 = eq_time(3)


disp(<span class="string">'Provar que para cada instante de t a sompa de pi(t) = 1:'</span>);
sum1 = sum(sum_pi(1,:))
sum2 = sum(sum_pi(2,:))
sum3 = sum(sum_pi(3,:))

<span class="keyword">for</span> j = 1:3
    figure(j)
    <span class="keyword">if</span> j == 1
        title(<span class="string">'Condi&ccedil;&atilde;o inicial -&gt; &acirc;ncora 7'</span>);
    <span class="keyword">elseif</span> j == 2
        title(<span class="string">'Condi&ccedil;&atilde;o inicial -&gt;&acirc;ncora 8'</span>);
    <span class="keyword">elseif</span> j == 3
        title(<span class="string">'Condi&ccedil;&atilde;o inicial -&gt; igual probabilidade para todas as &acirc;ncoras'</span>);
    <span class="keyword">end</span>
    xlabel(<span class="string">'Tempo'</span>);
    ylabel(<span class="string">'&Acirc;ncora'</span>);
    zlabel(<span class="string">'Probabilidade de ter o token'</span>);
<span class="keyword">end</span>
</pre><pre class="codeoutput">Tempo para atingir o equil&iacute;brio para cada condi&ccedil;&atilde;o:

condicao1 =

   325


condicao2 =

   435


condicao3 =

   376

Provar que para cada instante de t a sompa de pi(t) = 1:

sum1 =

   500


sum2 =

   500


sum3 =

   500

</pre><img vspace="5" hspace="5" src="Lab4_Final_04.png" alt=""> <img vspace="5" hspace="5" src="Lab4_Final_05.png" alt=""> <img vspace="5" hspace="5" src="Lab4_Final_06.png" alt=""> <p>Decidiu-se fazer tr&ecirc;s simula&ccedil;&otilde;es para estudar o impacto de <img src="Lab4_Final_eq12719111378361454579.png" alt="$\pi$">(0) na evolu&ccedil;&atilde;o do sistema. Escolhemos para a primeira condi&ccedil;&atilde;o o token estar come&ccedil;ar na &acirc;ncora 7, pois como conclu&iacute;do na quest&atilde;o 2.a esta &eacute; a &acirc;ncora que mais vezes tem o token por ter uma localiza&ccedil;&atilde;o central e v&aacute;rias liga&ccedil;&otilde;es, logo intuitivamente seria esta a condi&ccedil;&atilde;o inicial que conduziria a um tempo de equil&iacute;brio menor, como foi comprovado. Para a segunda situa&ccedil;&atilde;o escolhemos o oposto da primeira, ou seja, colocou-se o token na &acirc;ncora 8 que &eacute; a que possui menos vezes o token, logo ser&aacute; esta a que demora um maior tempo para atingir o equil&iacute;brio. Na terceira condi&ccedil;&atilde;o, todas as &acirc;ncoras t&ecirc;m uma igual probabilidade de possuir o token, logo o tempo para atingir o equil&iacute;brio &eacute; algo no meio da situa&ccedil;&atilde;o inicial 1 e 2. Para provar que para cada instante de <img src="Lab4_Final_eq12719111378361454579.png" alt="$\pi$">(t) a soma das suas probabilidades &eacute; 1 decidiu-se mostrar o somat&oacute;rio das probabilidades para todo o tempo , pois como o tempo &eacute; de 500, se o somat&oacute;rio for 500 &eacute; uma boa indica&ccedil;&atilde;o que est&aacute; tudo bem. Analisando a matriz sum_pi() pode-se confirmar isso, mas decidiu-se apresentar o resultado desta maneira para n&atilde;o saturar a Command Window.</p><h2 id="7">Exerc&iacute;cio 2.d</h2><pre class="codeinput">clear <span class="string">all</span>
close <span class="string">all</span>
clc

MarkovChain = load(<span class="string">'MarkovChain.mat'</span>);

worse_P = MarkovChain.P;

better_P = worse_P;

<span class="comment">%refazer liga&ccedil;oes de P para melhor</span>
better_P(2,6) = 0.3; better_P(2,4) = 0.35; better_P(2,13) = 0.35;
better_P(6,2) = 0.25; better_P(6,1) = 0.25; better_P(6,15) = 0.25; better_P(6,11) = 0.25;
better_P(3,19) = 0.5; better_P(3,12) = 0.5;
better_P(18,8) = 0.3; better_P(18,14) = 0.35; better_P(18, 16) = 0.35;
better_P(8,9) = 0.35; better_P(8,12) = 0.35; better_P(8,18) = 0.3;
better_P(20,5) = 0.25; better_P(20,1) = 0.25; better_P(20,7) = 0.25; better_P(20,14) = 0.25;


<span class="comment">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span>
<span class="comment">%calcular as probabilidades com a nova matriz de transi&ccedil;&otilde;es</span>

[vectors, values] = eig(better_P');
old_diference = 10000;
val_u = 1;
<span class="comment">%encontra o valor proprio mais perto de 1</span>
<span class="keyword">for</span> i = 1:size(values,1)
    diference = abs(values(i,i) - 1);
    <span class="keyword">if</span> diference &lt; old_diference
        old_diference = diference;
        value_index = i;
    <span class="keyword">end</span>
<span class="keyword">end</span>
<span class="comment">%vector a normalizar</span>
vec_n = vectors(:,value_index);
normalize = vec_n/sum(vec_n);
figure(1)
bar(normalize);
title(<span class="string">'Distribui&ccedil;&atilde;o de equil&iacute;brio da cadeia de Markov melhorada'</span>);
xlabel(<span class="string">'N&uacute;mero do estado da cadeia de Markov'</span>);
ylabel(<span class="string">'Probabilidade do token estar na posse do agente'</span>);
box <span class="string">on</span>;
grid <span class="string">on</span>;


<span class="comment">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span>
<span class="comment">%estimar a localiza&ccedil;&atilde;o da source</span>

sidelength = 100;
N = 20;
n = 2;
M = 100;
a = [MarkovChain.nodePos(:,2) , MarkovChain.nodePos(:,3)]';
x = MarkovChain.sourcePos';
D = squareform(pdist([x zeros(size(x)) a]'));
d = D(1,3:end);                     <span class="comment">% Source-anchor distances</span>
an = D(2,3:end);                    <span class="comment">% Anchor norms</span>
P0 = 100;
mu = 0;
sigma = 1e-2;
A = zeros(M,4);
b = zeros(M,1);
ni_space = [-0.1:.0002:0.1];
ni = normpdf(ni_space, mu, sigma);

number_meas = normalize.*M;
number_meas = round(number_meas);
j = 1;
<span class="keyword">for</span> i = 1:N
    meas = number_meas(i);
    <span class="keyword">if</span> meas &gt; 0
        better_P(j) = (P0./(d(i).^2))*exp(ni(j));
        A(j,1) = -2*better_P(j)*a(1,i);
        A(j,2) = -2*better_P(j)*a(2,i);
        A(j,3) = -1;
        A(j,4) = better_P(j);
        b(j) = -better_P(j)*(an(i)^2)';
        j = j + 1;
        meas = meas-1;
    <span class="keyword">end</span>
<span class="keyword">end</span>

z = A\b;
<span class="comment">% RLS formulation (one-shot)</span>
RlsPar = struct(<span class="string">'lam'</span>,1);
[e,w,RlsPar] = qrrls(A,b,RlsPar);
fprintf(<span class="string">'Erro com %d mudan&ccedil;as de agente na cadeia melhorada'</span>, M);
x_estimated = z(1:n);
error1 = norm(x-x_estimated)
<span class="comment">%error1 = norm(z-w)</span>
<span class="comment">%x_estimated = [w(1) w(2)]';</span>

figure(2)
plot(a'*[1; 1i],<span class="string">'o'</span>); hold <span class="string">all</span>
plot(x'*[1; 1i],<span class="string">'x'</span>); plot(x_estimated'*[1; 1i],<span class="string">'s'</span>); hold <span class="string">off</span>
axis(sidelength*[0 1 0 1]); axis(<span class="string">'square'</span>)
title(<span class="string">'Estima&ccedil;&atilde;o da posi&ccedil;&atilde;o da source com cadeia de Markov melhorada'</span>);
legend(<span class="string">'&Acirc;ncoras'</span>, <span class="string">'Posi&ccedil;&atilde;o real da source'</span>, <span class="string">'Posi&ccedil;&atilde;o estimada da source com 10000 mudan&ccedil;as'</span>);


<span class="comment">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span>
<span class="comment">%refazer liga&ccedil;&otilde;es de P para pior</span>
worse_P = worse_P;

worse_P(7,1) = 0.1; worse_P(7,20) = 0.7; worse_P(7,16) = 0.1; worse_P(7,19) = 0.1;
worse_P(19,3) = 0.1; worse_P(19,4) = 0.6; worse_P(19,13) = 0.2; worse_P(19,7) = 0.1;

<span class="comment">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span>
<span class="comment">%calcular as probabilidades com a nova matriz de transi&ccedil;&otilde;es</span>

[vectors, values] = eig(worse_P');
old_diference = 10000;
val_u = 1;
<span class="comment">%encontra o valor proprio mais perto de 1</span>
<span class="keyword">for</span> i = 1:size(values,1)
    diference = abs(values(i,i) - 1);
    <span class="keyword">if</span> diference &lt; old_diference
        old_diference = diference;
        value_index = i;
    <span class="keyword">end</span>
<span class="keyword">end</span>
<span class="comment">%vector a normalizar</span>
vec_n = vectors(:,value_index);
normalize = vec_n/sum(vec_n);
figure(3)
bar(normalize);
title(<span class="string">'Distribui&ccedil;&atilde;o de equil&iacute;brio da cadeia de Markov piorada'</span>);
xlabel(<span class="string">'N&uacute;mero do estado da cadeia de Markov'</span>);
ylabel(<span class="string">'Probabilidade do token estar na posse do agente'</span>);
box <span class="string">on</span>;
grid <span class="string">on</span>;

<span class="comment">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span>
<span class="comment">%estimar a localiza&ccedil;&atilde;o da source</span>

sidelength = 100;
N = 20;
n = 2;
M = 100;
a = [MarkovChain.nodePos(:,2) , MarkovChain.nodePos(:,3)]';
x = MarkovChain.sourcePos';
D = squareform(pdist([x zeros(size(x)) a]'));
d = D(1,3:end);                     <span class="comment">% Source-anchor distances</span>
an = D(2,3:end);                    <span class="comment">% Anchor norms</span>
P0 = 100;
mu = 0;
sigma = 1e-2;
A = zeros(M,4);
b = zeros(M,1);
ni_space = [-0.1:.0002:0.1];
ni = normpdf(ni_space, mu, sigma);

number_meas = normalize.*M;
number_meas = round(number_meas);
j = 1;
<span class="keyword">for</span> i = 1:N
    meas = number_meas(i);
    <span class="keyword">if</span> meas &gt; 0
        worse_P(j) = (P0./(d(i).^2))*exp(ni(j));
        A(j,1) = -2*worse_P(j)*a(1,i);
        A(j,2) = -2*worse_P(j)*a(2,i);
        A(j,3) = -1;
        A(j,4) = worse_P(j);
        b(j) = -worse_P(j)*(an(i)^2)';
        j = j + 1;
        meas = meas-1;
    <span class="keyword">end</span>
<span class="keyword">end</span>

z = A\b;
<span class="comment">% RLS formulation (one-shot)</span>
RlsPar = struct(<span class="string">'lam'</span>,1);
[e,w,RlsPar] = qrrls(A,b,RlsPar);
fprintf(<span class="string">'Erro com %d mudan&ccedil;as de agente'</span>, M);
x_estimated2 = z(1:n);
error2 = norm(x-x_estimated2)
<span class="comment">%error2 = norm(z-w)</span>
<span class="comment">%x_estimated2 = [w(1) w(2)]';</span>

figure(4)
plot(a'*[1; 1i],<span class="string">'o'</span>); hold <span class="string">all</span>
plot(x'*[1; 1i],<span class="string">'x'</span>); plot(x_estimated2'*[1; 1i],<span class="string">'s'</span>); hold <span class="string">off</span>
axis(sidelength*[0 1 0 1]); axis(<span class="string">'square'</span>)
title(<span class="string">'Estima&ccedil;&atilde;o da posi&ccedil;&atilde;o da source com cadeia de Markov piorada'</span>);
legend(<span class="string">'&Acirc;ncoras'</span>, <span class="string">'Posi&ccedil;&atilde;o real da source'</span>, <span class="string">'Posi&ccedil;&atilde;o estimada da source com 10000 mudan&ccedil;as'</span>);
</pre><pre class="codeoutput">Erro com 100 mudan&ccedil;as de agente na cadeia melhorada
error1 =

   1.0658e-14

Erro com 100 mudan&ccedil;as de agente
error2 =

   1.5888e-14

</pre><img vspace="5" hspace="5" src="Lab4_Final_07.png" alt=""> <img vspace="5" hspace="5" src="Lab4_Final_08.png" alt=""> <img vspace="5" hspace="5" src="Lab4_Final_09.png" alt=""> <img vspace="5" hspace="5" src="Lab4_Final_10.png" alt=""> <p>Olhando para a matriz de transmiss&atilde;o P &eacute; possivel concluir algumas coisas. Em primeiro, destaca-se a olho a exist&ecirc;ncia de dois grandes subgrupos, isto &eacute;, o grupo do lado esquerdo da liga&ccedil;&atilde;o 7-19 e o grupo do lado direito desta liga&ccedil;&atilde;o. Ambos estes grafos t&ecirc;m uma probabilidade de 0.25 de transitarem para o outro, o que mostra como pode ser dif&iacute;cil para o token passar de um lado para o outro, pois estes grafos s&atilde;o os &uacute;nicos pontos de liga&ccedil;&atilde;o de um sub-grupo para o outro. Mesmo dentro destes sub-grupos o token n&atilde;o percorre os grafos de maneira equitativa. A ponto de refer&ecirc;ncia referimos por exemplo que a probabilidade do token passar do grafo 3 para o 12 &eacute; de 0.1, ou que para passar do 1 para o 6 &eacute; de 0.2, sendo que estes dois grafos s&atilde;o a &uacute;nica liga&ccedil;&atilde;o para alguns grafos. Portanto, &eacute; necess&aacute;rio refazer a cadeia de Markov para o token poder passar por mais agentes em vez de passar bastante tempo na posse dos mesmos agentes. Observando as primeiras duas figuras conlu&iacute;mos que a distribui&ccedil;&atilde;o dos estados mais e menos prov&aacute;vel est&aacute; bastante mais equilibrada que no exerc&iacute;cio 2.a, e que o erro da cadeia melhorada tamb&eacute;m &eacute; um pouco melhor, Decidimos reduzir o n&uacute;mero de transi&ccedil;&otilde;es de 10000 (2.b) para 100, de forma a podermos observar diferen&ccedil;a na estima&ccedil;&atilde;o da source, pois para 10000 transi&ccedil;&otilde;es ambas as cadeias davam o mesmo erro.</p><h2 id="9">Exerc&iacute;cio 3a e 3b</h2><pre class="codeinput">part3Exe(1000, 1000, 0, 1);
</pre><img vspace="5" hspace="5" src="Lab4_Final_11.png" alt=""> <img vspace="5" hspace="5" src="Lab4_Final_12.png" alt=""> <img vspace="5" hspace="5" src="Lab4_Final_13.png" alt=""> <img vspace="5" hspace="5" src="Lab4_Final_14.png" alt=""> <p>Conseguimos verificar que a distribui&ccedil;&atilde;o dos estados e a frequ&ecirc;ncia dos mesmos corresponde ao obtido na sec&ccedil;&atilde;o 2. Obtiv&eacute;mos o mesmo gr&aacute;fico de barras para a distribui&ccedil;&atilde;o de equilibrio da cadeia de Markov. Conseguimos concluir tamb&eacute;m que quanto mais alto o n&uacute;mero de runs ou de instantes de tempo que se quer analisar, melhores ser&atilde;o os resultados, e o tempo de converg&ecirc;ncia reduz-se. Isto &eacute; uma das caracter&iacute;sticas do algoritmo de estima&ccedil;&atilde;o de Monte Carlo.</p><h2 id="11">Exerc&iacute;cio 3c</h2><pre class="codeinput">part3Exe(1000, 1000, 1, 1);  <span class="comment">%lambda = 1</span>
</pre><img vspace="5" hspace="5" src="Lab4_Final_15.png" alt=""> <img vspace="5" hspace="5" src="Lab4_Final_16.png" alt=""> <img vspace="5" hspace="5" src="Lab4_Final_17.png" alt=""> <img vspace="5" hspace="5" src="Lab4_Final_18.png" alt=""> <h2 id="12">Functions - part3Exe</h2><pre class="codeinput">type(<span class="string">'part3Exe.m'</span>)
</pre><pre class="codeoutput">
function part3Exe(n_runs, steps, slow, lambda)
info = importdata('MarkovChain.mat');        %information about the problem
P = info.P;
info.nodePos(:,1) = [];
npos = info.nodePos';   %a
spos = info.sourcePos'; %x
n_anchor = length(info.P);                           %number of steps on the Markov
tok_node = ceil((rand()*19)+1);                      %generate random initial condition


[xe, esterror, error_zw, error, vec_w, statepath, poserror] = execute(spos, npos, P, tok_node, n_runs, steps, slow, lambda);
mode_state = mode(statepath(:));        %most frequent state

for i =1:length(npos)                   %creates ocurrences vector
    occur(i) = nnz(statepath==i);
end

occur = occur/sum(occur);

% Plots
figure(1)
plot(1:steps, error, '-b');
grid on
title('Qrrls Error');
xlabel('Time Steps');
ylabel('Error');

figure(2)
plot(1:steps, error_zw, '-r');
grid on
title('Z-W Error');
xlabel('Time Steps');
ylabel('Error');

figure(3)
plot(1:steps, poserror, '-g');
grid on
title('Source Position - W Error');
xlabel('Time Steps');
ylabel('Error');

figure(4)
bar(occur);
grid on
title('Ocurrence probability bar chart');
xlabel('States');
ylabel('Probability');
end

</pre><h2 id="13">Functions - execute</h2><pre class="codeinput">type(<span class="string">'execute.m'</span>)
</pre><pre class="codeoutput">
function [xe, poserr, error_zw, error, vec_w, statepath, poserror] = execute(spos, npos, P, tok_node, n_runs, steps, slow, lambda)

[Powr, an] = PowrGen(spos, npos);

h = waitbar(0,'Please wait...');
for n = 1:n_runs
    for c = 1:steps
        if slow == 1  %if slow mode activated
            spos = spos+0.1; %moves per interation
           [Powr, an] = PowrGen(spos, npos);  %new powr
        end
        tok_node = find(cumsum([P(tok_node,:)]) &gt; rand,1,'first');          %next node
        statepath(n,c) = tok_node;
        % Localize source by least-squares
        A(c,:) = [-2*repmat(Powr(1,tok_node),[2 1]).*npos(:,tok_node); -1; Powr(1,tok_node)]';
        b(c,:) = (-Powr(1,tok_node)*norm(an(tok_node))^2)';
        
        z = A\b;
        xe(1,c) = z(1);
        xe(2,c) = z(2);
        poserr(c) = norm(spos-xe);             %estimation error
        
        % RLS formulation (incremental)
        RlsPar = struct('lam',lambda);
        
        [e,w,RlsPar] = qrrls(A(c,:),b(c),RlsPar);
        error(c) = abs(e);
        vec_w(1,c) = w(1);
        vec_w(2,c) = w(2);
        error_zw(c) = norm(z-w);
        poserror(c) = norm(spos-w(1:2));
        
    end
    waitbar(n/n_runs);
end
close(h);

end

</pre><h2 id="14">Functions - PowrGen</h2><pre class="codeinput">type(<span class="string">'PowrGen.m'</span>)
</pre><pre class="codeoutput">
function [Powr, an] = PowrGen(spos,npos)
D = squareform(pdist([spos zeros(size(spos)) npos]'));
d = D(1,3:end);				% Source-anchor distances
an = D(2,3:end);			% Anchor norms

% Generate observations
Pow0 = 100;				% Source power
Powr = Pow0./(d.^2);				% Noiseless RSSI

stdev = 1e-1;				% Log-noise standard deviation
%stdev = 0;
Powr = Powr.*exp(stdev*randn(size(Powr)));	% Introduce noise
QP = 1e-2;
Powr = QP*round(Powr/QP);			% Quantize power measurements
end

</pre><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2018a</a><br></p></div><!--
##### SOURCE BEGIN #####
%%
% LAB 4 - MSIM
% Autor: Bernardo Rocha & Vasco Araújo
% Número de Aluno: 89867 & 90817
% Junho 2020; Última Revisão: 03/06/2020   

%% Exercício 2.a

close all
clear all
clc

MarkovChain = load('MarkovChain.mat');

P = MarkovChain.P;

[vectors, values] = eig(P');

old_diference = 10000;
val_u = 1;
%encontra o valor proprio mais perto de 1
for i = 1:size(values,1)
    diference = abs(values(i,i) - 1);
    if diference < old_diference
        old_diference = diference;
        value_index = i;
    end
end

%vector a normalizar
vec_n = vectors(:,value_index);

normalize = vec_n/sum(vec_n);

%se esta tudo bem soma das probabilidades tem que dar 1
prob_sum = sum(normalize)

%faz gráfico de barras
figure(1)
bar(normalize);
title('Distribuição de equilíbrio da cadeia de Markov');
xlabel('Número do estado da cadeia de Markov');
ylabel('Probabilidade do token estar na posse do agente');
box on;
grid on;

%encontra estado mais provavel
high_prob = 0;
low_prob = 1;
%encontra o 1º estado mais provavel e o 1º estado menos provavel
for i = 1:size(normalize,1)
    if normalize(i) > high_prob
        high_prob = normalize(i);
    end
    if normalize(i) <= low_prob
        low_prob = normalize(i);
    end
end

%volta a correr para ver se ha mais estados com mesma probabilidade que o
%mais provavel e o menos provavel
h = 1;
l = 1;
for i = 1:size(normalize,1)
    if normalize(i) == high_prob
        highest_prob_index(h) = i;
        highest_prob(h) = normalize(i)
        highest_prob_anchor = i
        h = h + 1;
    end
    if normalize(i) == low_prob
        lowest_prob_index(l) = i;
        lowest_prob(l) = normalize(i)
        lowest_prob_anchor = i
    end
end



%%
% Olhando para o gráfico de barras poderia pensar-se que os estados mais
% prováveis eram os estados 7 e 19, no entanto correndo o código para
% encontrar os maiores estados apenas no dá o estado 7. Isto é porque,
% analisando os valores de normalize verifica-se que o estado 7 tem uma
% probabilidade de 0.096493187402279 e o estado 19 uma probabilidade de
% 0.096493187402278, por isso o código apenas retorna 7 como o estado
% com maior probabilidade. O mesmo acontece com os estados 8 e 17 para o
% cálculo do estado com menor probabilidade, visto que o estado 17 tem uma 
% probabilidade um pouco menor. Assim sendo, se quiseremos ser rigorosos o
% estado com maior probabilidade é o estado 7 e o estado com menor 
% probabilidade é o estado 8. 



        

%% Exercício 2.b

close all
clear all
clc

MarkovChain = load('MarkovChain.mat');

%parâmetros da simulação
N = size(MarkovChain.nodePos,1);        %numero de ancoras
n = 2;                                  %numero de dimensoes
sidelength = 100;  

M = 10000;                               %numero de observações

%posiçoes das âncoras
a = [MarkovChain.nodePos(:,2) , MarkovChain.nodePos(:,3)]';
%posições das sources
x = MarkovChain.sourcePos';


D = squareform(pdist([x zeros(size(x)) a]'));
d = D(1,3:end);                     % Source-anchor distances
an = D(2,3:end);                    % Anchor norms
    
P0 = 100;                    
mu = 0;
sigma = 1e-2;                       

%inicializa matrizes
A = zeros(M,4);
b = zeros(M,1);

%ruido ni
ni_space = [-0.1:.0002:0.1];
ni = normpdf(ni_space, mu, sigma);

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%código da 2.a
P = MarkovChain.P;
[vectors, values] = eig(P');
old_diference = 10000;
val_u = 1;
%encontra o valor proprio mais perto de 1
for i = 1:size(values,1)
    diference = abs(values(i,i) - 1);
    if diference < old_diference
        old_diference = diference;
        value_index = i;
    end
end
%vector a normalizar
vec_n = vectors(:,value_index);
normalize = vec_n/sum(vec_n);
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%numero de vezes que cada âncora tem o token
number_meas = normalize.*M; 
%aproximação para o número inteiro mais próximo
number_meas = round(number_meas);

%calcula as matrizes A e b
j = 1;
for i = 1:N
    meas = number_meas(i);
    if meas ~= 0
        P(j) = (P0./(d(i).^2))*exp(ni(j));
        A(j,1) = -2*P(j)*a(1,i);
        A(j,2) = -2*P(j)*a(2,i);
        A(j,3) = -1;
        A(j,4) = P(j);
        b(j) = -P(j)*(an(i)^2)';
        j = j + 1;
        meas = meas-1;
    end
end

z = A\b;
fprintf('Erro com %d mudanças de agente', M);
x_estimated = z(1:n);
error1 = norm(x-x_estimated)

% RLS formulation (one-shot)
% RlsPar = struct('lam',1);
% [e,w,RlsPar] = qrrls(A,b,RlsPar);
%error1 = norm(z-w)
%x_estimated = [w(1) w(2)]';



figure(1)
plot(a'*[1; 1i],'o'); hold all
plot(x'*[1; 1i],'x');plot(x_estimated'*[1; 1i],'s'); hold off
axis(sidelength*[0 1 0 1]); axis('square')
title('Estimação da posição da source');
legend('Âncoras', 'Posição real da source', 'Posição estimada da source com 10000 mudanças');
xlabel('x');
ylabel('y');

figure(2)
hold all
plot(x'*[1; 1i],'x'); plot(x_estimated'*[1; 1i],'s'); 
axis([84.9999 85.0001 29.9999 30.0001]);
title('Estimação da posição da source ampliada');
legend('Posição real da source', 'Posição estimada da source com 10000 mudanças', 'Location','southwest');
xlabel('x');
ylabel('y');


%%
% Decidiu-se usar a primeira formulação apresentada no ficheiro 'rssiloc.m' por
% ser a que apresentava um erro menor. Usando o algoritmo de mínimos quedrados
% com base na solução matricial consegue-se um
% resultado bastante fiel para a fonte estimada, dado um número elevado de
% transições. O erro nunca poderia ser zero devido ao ruído gaussiano na
% medição da potência da fonte, por isso consideramos o nosso erro de
% 1.0658e-14 bastante bom. 




%% Exercício 2.c

clear all
close all
clc



MarkovChain = load('MarkovChain.mat');
P = MarkovChain.P;

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2.a (para usar mais a frente)
[vectors, values] = eig(P');
old_diference = 10000;
val_u = 1;
%encontra o valor proprio mais perto de 1
for i = 1:size(values,1)
    diference = abs(values(i,i) - 1);
    if diference < old_diference
        old_diference = diference;
        value_index = i;
    end
end
%vector a normalizar
vec_n = vectors(:,value_index);
normalize = vec_n/sum(vec_n);
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

N = 20;
time = 500;

pi_array = zeros(N,time);

%inicializar a matriz pi_array_0
%cada coluna corresponde ao vetor inicial para cada simulação
pi_array_0 = zeros(N,4);

%1º condição inicial -> começa no estado 7
pi_array_0(7,1) = 1;
%2º condição inicial -> começa no estado 8
pi_array_0(8,2) = 1;
%3º condição -> todos os estados têm a mesma probabilidade
for i = 1:20
    pi_array_0(i,3) = 1/20;
end

for i = 1:3
    eq_time(i) = 0;
end


for j = 1:3
    pi_array(:,1) = pi_array_0(:,j);
    for i = 2:time
        pi_array(:,i) = pi_array(:,1)'*P^(i);
    end
    
    %para comparar com os resultados da decomposição em valores e vetores
    %proprios (2.a) vamos calcular o tempo que demora a atingir o
    %equilíbrio
    for i = 1:time
        if pi_array(:,i) - normalize < 0.000001
            eq_time(j) = i;
            break;
        end
    end
    
    figure(j)
    t_array = 1:1:time;
    anc=repmat(1:1:20, length(t_array), 1);
    plot3(t_array, anc, pi_array);
end

%verificar que para cada t soma de pi(t)= 1
for j = 1:3
    for i = 1:time
        sum_pi(j,i) = sum(pi_array(:,i));
    end
end

disp('Tempo para atingir o equilíbrio para cada condição:');
condicao1 = eq_time(1)
condicao2 = eq_time(2)
condicao3 = eq_time(3)


disp('Provar que para cada instante de t a sompa de pi(t) = 1:');
sum1 = sum(sum_pi(1,:))
sum2 = sum(sum_pi(2,:))
sum3 = sum(sum_pi(3,:))

for j = 1:3
    figure(j)
    if j == 1
        title('Condição inicial -> âncora 7');
    elseif j == 2
        title('Condição inicial ->âncora 8');
    elseif j == 3
        title('Condição inicial -> igual probabilidade para todas as âncoras');
    end
    xlabel('Tempo');
    ylabel('Âncora');
    zlabel('Probabilidade de ter o token');
end

%%
% Decidiu-se fazer três simulações para estudar o impacto de $\pi$(0) na
% evolução do sistema. Escolhemos para a primeira condição o token estar
% começar na âncora 7, pois como concluído na questão 2.a esta é a âncora
% que mais vezes tem o token por ter uma localização central e várias
% ligações, logo intuitivamente seria esta a condição inicial que
% conduziria a um tempo de equilíbrio menor, como foi comprovado. Para a
% segunda situação escolhemos o oposto da primeira, ou seja, colocou-se o
% token na âncora 8 que é a que possui menos vezes o token, logo será esta
% a que demora um maior tempo para atingir o equilíbrio. Na terceira
% condição, todas as âncoras têm uma igual probabilidade de possuir o
% token, logo o tempo para atingir o equilíbrio é algo no meio da situação
% inicial 1 e 2.
% Para provar que para cada instante de $\pi$(t) a soma das suas
% probabilidades é 1 decidiu-se mostrar o somatório das probabilidades para
% todo o tempo , pois como o tempo é de 500, se o somatório for 500 é uma boa
% indicação que está tudo bem. Analisando a matriz sum_pi() pode-se
% confirmar isso, mas decidiu-se apresentar o resultado desta maneira para
% não saturar a Command Window.


%% Exercício 2.d

clear all
close all
clc

MarkovChain = load('MarkovChain.mat');

worse_P = MarkovChain.P;

better_P = worse_P;

%refazer ligaçoes de P para melhor
better_P(2,6) = 0.3; better_P(2,4) = 0.35; better_P(2,13) = 0.35;
better_P(6,2) = 0.25; better_P(6,1) = 0.25; better_P(6,15) = 0.25; better_P(6,11) = 0.25;
better_P(3,19) = 0.5; better_P(3,12) = 0.5;
better_P(18,8) = 0.3; better_P(18,14) = 0.35; better_P(18, 16) = 0.35;
better_P(8,9) = 0.35; better_P(8,12) = 0.35; better_P(8,18) = 0.3;
better_P(20,5) = 0.25; better_P(20,1) = 0.25; better_P(20,7) = 0.25; better_P(20,14) = 0.25;


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%calcular as probabilidades com a nova matriz de transições

[vectors, values] = eig(better_P');
old_diference = 10000;
val_u = 1;
%encontra o valor proprio mais perto de 1
for i = 1:size(values,1)
    diference = abs(values(i,i) - 1);
    if diference < old_diference
        old_diference = diference;
        value_index = i;
    end
end
%vector a normalizar
vec_n = vectors(:,value_index);
normalize = vec_n/sum(vec_n);
figure(1)
bar(normalize);
title('Distribuição de equilíbrio da cadeia de Markov melhorada');
xlabel('Número do estado da cadeia de Markov');
ylabel('Probabilidade do token estar na posse do agente');
box on;
grid on;


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%estimar a localização da source

sidelength = 100;
N = 20;
n = 2;
M = 100;
a = [MarkovChain.nodePos(:,2) , MarkovChain.nodePos(:,3)]';
x = MarkovChain.sourcePos';
D = squareform(pdist([x zeros(size(x)) a]'));
d = D(1,3:end);                     % Source-anchor distances
an = D(2,3:end);                    % Anchor norms
P0 = 100;                    
mu = 0;
sigma = 1e-2;                       
A = zeros(M,4);
b = zeros(M,1);
ni_space = [-0.1:.0002:0.1];
ni = normpdf(ni_space, mu, sigma);

number_meas = normalize.*M; 
number_meas = round(number_meas);
j = 1;
for i = 1:N
    meas = number_meas(i);
    if meas > 0
        better_P(j) = (P0./(d(i).^2))*exp(ni(j));
        A(j,1) = -2*better_P(j)*a(1,i);
        A(j,2) = -2*better_P(j)*a(2,i);
        A(j,3) = -1;
        A(j,4) = better_P(j);
        b(j) = -better_P(j)*(an(i)^2)';
        j = j + 1;
        meas = meas-1;
    end
end

z = A\b;
% RLS formulation (one-shot)
RlsPar = struct('lam',1);
[e,w,RlsPar] = qrrls(A,b,RlsPar);
fprintf('Erro com %d mudanças de agente na cadeia melhorada', M);
x_estimated = z(1:n);
error1 = norm(x-x_estimated)
%error1 = norm(z-w)
%x_estimated = [w(1) w(2)]';

figure(2)
plot(a'*[1; 1i],'o'); hold all
plot(x'*[1; 1i],'x'); plot(x_estimated'*[1; 1i],'s'); hold off
axis(sidelength*[0 1 0 1]); axis('square')
title('Estimação da posição da source com cadeia de Markov melhorada');
legend('Âncoras', 'Posição real da source', 'Posição estimada da source com 10000 mudanças');


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%refazer ligações de P para pior
worse_P = worse_P;

worse_P(7,1) = 0.1; worse_P(7,20) = 0.7; worse_P(7,16) = 0.1; worse_P(7,19) = 0.1;
worse_P(19,3) = 0.1; worse_P(19,4) = 0.6; worse_P(19,13) = 0.2; worse_P(19,7) = 0.1;

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%calcular as probabilidades com a nova matriz de transições

[vectors, values] = eig(worse_P');
old_diference = 10000;
val_u = 1;
%encontra o valor proprio mais perto de 1
for i = 1:size(values,1)
    diference = abs(values(i,i) - 1);
    if diference < old_diference
        old_diference = diference;
        value_index = i;
    end
end
%vector a normalizar
vec_n = vectors(:,value_index);
normalize = vec_n/sum(vec_n);
figure(3)
bar(normalize);
title('Distribuição de equilíbrio da cadeia de Markov piorada');
xlabel('Número do estado da cadeia de Markov');
ylabel('Probabilidade do token estar na posse do agente');
box on;
grid on;

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%estimar a localização da source

sidelength = 100;
N = 20;
n = 2;
M = 100;
a = [MarkovChain.nodePos(:,2) , MarkovChain.nodePos(:,3)]';
x = MarkovChain.sourcePos';
D = squareform(pdist([x zeros(size(x)) a]'));
d = D(1,3:end);                     % Source-anchor distances
an = D(2,3:end);                    % Anchor norms
P0 = 100;                    
mu = 0;
sigma = 1e-2;                       
A = zeros(M,4);
b = zeros(M,1);
ni_space = [-0.1:.0002:0.1];
ni = normpdf(ni_space, mu, sigma);

number_meas = normalize.*M; 
number_meas = round(number_meas);
j = 1;
for i = 1:N
    meas = number_meas(i);
    if meas > 0
        worse_P(j) = (P0./(d(i).^2))*exp(ni(j));
        A(j,1) = -2*worse_P(j)*a(1,i);
        A(j,2) = -2*worse_P(j)*a(2,i);
        A(j,3) = -1;
        A(j,4) = worse_P(j);
        b(j) = -worse_P(j)*(an(i)^2)';
        j = j + 1;
        meas = meas-1;
    end
end

z = A\b;
% RLS formulation (one-shot)
RlsPar = struct('lam',1);
[e,w,RlsPar] = qrrls(A,b,RlsPar);
fprintf('Erro com %d mudanças de agente', M);
x_estimated2 = z(1:n);
error2 = norm(x-x_estimated2)
%error2 = norm(z-w)
%x_estimated2 = [w(1) w(2)]';

figure(4)
plot(a'*[1; 1i],'o'); hold all
plot(x'*[1; 1i],'x'); plot(x_estimated2'*[1; 1i],'s'); hold off
axis(sidelength*[0 1 0 1]); axis('square')
title('Estimação da posição da source com cadeia de Markov piorada');
legend('Âncoras', 'Posição real da source', 'Posição estimada da source com 10000 mudanças');
%%
% Olhando para a matriz de transmissão P é possivel concluir algumas
% coisas. Em primeiro, destaca-se a olho a existência de dois grandes
% subgrupos, isto é, o grupo do lado esquerdo da ligação 7-19 e o grupo do 
% lado direito desta ligação. Ambos estes grafos têm uma probabilidade de
% 0.25 de transitarem para o outro, o que mostra como pode ser difícil para
% o token passar de um lado para o outro, pois estes grafos são os únicos
% pontos de ligação de um sub-grupo para o outro. Mesmo dentro destes
% sub-grupos o token não percorre os grafos de maneira equitativa. A ponto
% de referência referimos por exemplo que a probabilidade do token passar
% do grafo 3 para o 12 é de 0.1, ou que para passar do 1 para o 6 é de 0.2,
% sendo que estes dois grafos são a única ligação para alguns grafos.
% Portanto, é necessário refazer a cadeia de Markov para o token poder
% passar por mais agentes em vez de passar bastante tempo na posse dos
% mesmos agentes.
% Observando as primeiras duas figuras conluímos que a distribuição dos
% estados mais e menos provável está bastante mais equilibrada que no
% exercício 2.a, e que o erro da cadeia melhorada também é um pouco melhor,
% Decidimos reduzir o número de transições de 10000 (2.b) para 100, de forma
% a podermos observar diferença na estimação da source, pois para 10000
% transições ambas as cadeias davam o mesmo erro.

%% Exercício 3a e 3b
part3Exe(1000, 1000, 0, 1);

%%
% Conseguimos verificar que a distribuição dos estados e a frequência dos
% mesmos corresponde ao obtido na secção 2. Obtivémos o mesmo gráfico de
% barras para a distribuição de equilibrio da cadeia de Markov.
% Conseguimos concluir também que quanto mais alto o número de runs ou de
% instantes de tempo que se quer analisar, melhores serão os resultados, e
% o tempo de convergência reduz-se. Isto é uma das características do
% algoritmo de estimação de Monte Carlo.
%

%% Exercício 3c
part3Exe(1000, 1000, 1, 1);  %lambda = 1

%% Functions - part3Exe
type('part3Exe.m')

%% Functions - execute
type('execute.m')

%% Functions - PowrGen
type('PowrGen.m')

        


##### SOURCE END #####
--></body></html>